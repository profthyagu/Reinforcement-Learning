{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import EpsGreedyQPolicy\n",
    "from rl.memory import SequentialMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "ENV_NAME = 'CartPole-v0'\n",
    "\n",
    "# Get the environment and extract the number of actions available in the Cartpole problem\n",
    "env = gym.make(ENV_NAME)\n",
    "np.random.seed(123)\n",
    "env.seed(123)\n",
    "nb_actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_1 (Flatten)          (None, 4)                 0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                80        \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 34        \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 114\n",
      "Trainable params: 114\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=(1,) + env.observation_space.shape))\n",
    "model.add(Dense(16))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(nb_actions))\n",
    "model.add(Activation('linear'))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 1000 steps ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\Anaconda3\\lib\\site-packages\\rl\\memory.py:39: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
      "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  79/1000: episode: 1, duration: 2.397s, episode steps: 79, steps per second: 33, episode reward: 79.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: 0.060 [-0.402, 0.722], loss: 0.426330, mean_absolute_error: 0.495163, mean_q: 0.053828\n",
      " 113/1000: episode: 2, duration: 0.565s, episode steps: 34, steps per second: 60, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: 0.151 [-0.159, 0.753], loss: 0.350759, mean_absolute_error: 0.444407, mean_q: 0.193362\n",
      " 163/1000: episode: 3, duration: 0.832s, episode steps: 50, steps per second: 60, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.082 [-0.295, 0.778], loss: 0.311264, mean_absolute_error: 0.462657, mean_q: 0.321871\n",
      " 197/1000: episode: 4, duration: 0.565s, episode steps: 34, steps per second: 60, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: 0.081 [-0.228, 0.770], loss: 0.275150, mean_absolute_error: 0.500660, mean_q: 0.474062\n",
      " 261/1000: episode: 5, duration: 1.067s, episode steps: 64, steps per second: 60, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.004 [-0.394, 0.861], loss: 0.231492, mean_absolute_error: 0.562896, mean_q: 0.679895\n",
      " 295/1000: episode: 6, duration: 0.564s, episode steps: 34, steps per second: 60, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.093 [-0.207, 0.836], loss: 0.173806, mean_absolute_error: 0.634566, mean_q: 0.936653\n",
      " 328/1000: episode: 7, duration: 0.549s, episode steps: 33, steps per second: 60, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.076 [-0.348, 0.908], loss: 0.135265, mean_absolute_error: 0.702516, mean_q: 1.148234\n",
      " 354/1000: episode: 8, duration: 0.434s, episode steps: 26, steps per second: 60, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.089 [-0.396, 0.810], loss: 0.120205, mean_absolute_error: 0.792136, mean_q: 1.350701\n",
      " 375/1000: episode: 9, duration: 0.348s, episode steps: 21, steps per second: 60, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: 0.110 [-0.542, 0.947], loss: 0.096788, mean_absolute_error: 0.852513, mean_q: 1.538060\n",
      " 391/1000: episode: 10, duration: 0.267s, episode steps: 16, steps per second: 60, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.088 [-0.609, 1.077], loss: 0.086663, mean_absolute_error: 0.912015, mean_q: 1.680550\n",
      " 415/1000: episode: 11, duration: 0.397s, episode steps: 24, steps per second: 60, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.068 [-0.627, 1.205], loss: 0.083227, mean_absolute_error: 0.973013, mean_q: 1.850057\n",
      " 436/1000: episode: 12, duration: 0.349s, episode steps: 21, steps per second: 60, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: 0.089 [-0.369, 1.041], loss: 0.087186, mean_absolute_error: 1.077275, mean_q: 2.037127\n",
      " 459/1000: episode: 13, duration: 0.383s, episode steps: 23, steps per second: 60, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: 0.085 [-0.543, 1.001], loss: 0.091432, mean_absolute_error: 1.171083, mean_q: 2.230646\n",
      " 482/1000: episode: 14, duration: 0.382s, episode steps: 23, steps per second: 60, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: 0.080 [-0.394, 1.042], loss: 0.091013, mean_absolute_error: 1.251087, mean_q: 2.419195\n",
      " 496/1000: episode: 15, duration: 0.234s, episode steps: 14, steps per second: 60, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: 0.094 [-0.607, 1.237], loss: 0.115860, mean_absolute_error: 1.344404, mean_q: 2.574838\n",
      " 509/1000: episode: 16, duration: 0.215s, episode steps: 13, steps per second: 60, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.385 [0.000, 1.000], mean observation: 0.106 [-0.775, 1.270], loss: 0.127558, mean_absolute_error: 1.387705, mean_q: 2.681187\n",
      " 526/1000: episode: 17, duration: 0.282s, episode steps: 17, steps per second: 60, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.412 [0.000, 1.000], mean observation: 0.072 [-0.775, 1.421], loss: 0.151920, mean_absolute_error: 1.459794, mean_q: 2.813784\n",
      " 543/1000: episode: 18, duration: 0.282s, episode steps: 17, steps per second: 60, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.412 [0.000, 1.000], mean observation: 0.076 [-0.800, 1.391], loss: 0.130124, mean_absolute_error: 1.533247, mean_q: 2.975498\n",
      " 557/1000: episode: 19, duration: 0.233s, episode steps: 14, steps per second: 60, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.357 [0.000, 1.000], mean observation: 0.124 [-0.770, 1.523], loss: 0.168292, mean_absolute_error: 1.623166, mean_q: 3.157410\n",
      " 568/1000: episode: 20, duration: 0.182s, episode steps: 11, steps per second: 61, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.120 [-0.981, 1.729], loss: 0.177897, mean_absolute_error: 1.652449, mean_q: 3.246284\n",
      " 578/1000: episode: 21, duration: 0.165s, episode steps: 10, steps per second: 61, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.300 [0.000, 1.000], mean observation: 0.146 [-0.771, 1.498], loss: 0.173126, mean_absolute_error: 1.744032, mean_q: 3.364439\n",
      " 590/1000: episode: 22, duration: 0.200s, episode steps: 12, steps per second: 60, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.113 [-0.805, 1.493], loss: 0.173703, mean_absolute_error: 1.784933, mean_q: 3.469061\n",
      " 600/1000: episode: 23, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.300 [0.000, 1.000], mean observation: 0.135 [-0.756, 1.495], loss: 0.150852, mean_absolute_error: 1.813883, mean_q: 3.588907\n",
      " 611/1000: episode: 24, duration: 0.183s, episode steps: 11, steps per second: 60, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.364 [0.000, 1.000], mean observation: 0.127 [-0.962, 1.474], loss: 0.254361, mean_absolute_error: 1.914610, mean_q: 3.713852\n",
      " 626/1000: episode: 25, duration: 0.249s, episode steps: 15, steps per second: 60, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.068 [-0.999, 1.674], loss: 0.295237, mean_absolute_error: 1.961449, mean_q: 3.812058\n",
      " 636/1000: episode: 26, duration: 0.165s, episode steps: 10, steps per second: 61, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.300 [0.000, 1.000], mean observation: 0.120 [-0.776, 1.493], loss: 0.377689, mean_absolute_error: 2.040911, mean_q: 3.863454\n",
      " 648/1000: episode: 27, duration: 0.200s, episode steps: 12, steps per second: 60, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.085 [-0.992, 1.574], loss: 0.298325, mean_absolute_error: 2.075009, mean_q: 3.948087\n",
      " 659/1000: episode: 28, duration: 0.182s, episode steps: 11, steps per second: 60, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.364 [0.000, 1.000], mean observation: 0.100 [-0.838, 1.450], loss: 0.227776, mean_absolute_error: 2.104806, mean_q: 4.071099\n",
      " 673/1000: episode: 29, duration: 0.232s, episode steps: 14, steps per second: 60, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.357 [0.000, 1.000], mean observation: 0.126 [-0.749, 1.514], loss: 0.284499, mean_absolute_error: 2.150162, mean_q: 4.230255\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 684/1000: episode: 30, duration: 0.183s, episode steps: 11, steps per second: 60, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.120 [-1.134, 1.941], loss: 0.502934, mean_absolute_error: 2.302229, mean_q: 4.359598\n",
      " 697/1000: episode: 31, duration: 0.213s, episode steps: 13, steps per second: 61, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.308 [0.000, 1.000], mean observation: 0.100 [-1.017, 1.716], loss: 0.457887, mean_absolute_error: 2.297588, mean_q: 4.373185\n",
      " 708/1000: episode: 32, duration: 0.184s, episode steps: 11, steps per second: 60, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.364 [0.000, 1.000], mean observation: 0.105 [-1.023, 1.554], loss: 0.563538, mean_absolute_error: 2.400669, mean_q: 4.474579\n",
      " 720/1000: episode: 33, duration: 0.200s, episode steps: 12, steps per second: 60, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.113 [-0.769, 1.442], loss: 0.482886, mean_absolute_error: 2.428541, mean_q: 4.575483\n",
      " 733/1000: episode: 34, duration: 0.216s, episode steps: 13, steps per second: 60, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.385 [0.000, 1.000], mean observation: 0.117 [-0.742, 1.466], loss: 0.567559, mean_absolute_error: 2.498142, mean_q: 4.655968\n",
      " 743/1000: episode: 35, duration: 0.161s, episode steps: 10, steps per second: 62, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.300 [0.000, 1.000], mean observation: 0.121 [-0.815, 1.506], loss: 0.514055, mean_absolute_error: 2.510551, mean_q: 4.765946\n",
      " 754/1000: episode: 36, duration: 0.183s, episode steps: 11, steps per second: 60, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.109 [-1.391, 2.133], loss: 0.471909, mean_absolute_error: 2.541616, mean_q: 4.865230\n",
      " 771/1000: episode: 37, duration: 0.283s, episode steps: 17, steps per second: 60, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.412 [0.000, 1.000], mean observation: 0.071 [-0.771, 1.332], loss: 0.482100, mean_absolute_error: 2.584657, mean_q: 4.976024\n",
      " 781/1000: episode: 38, duration: 0.165s, episode steps: 10, steps per second: 60, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.300 [0.000, 1.000], mean observation: 0.120 [-0.772, 1.472], loss: 0.615146, mean_absolute_error: 2.677366, mean_q: 5.078214\n",
      " 795/1000: episode: 39, duration: 0.232s, episode steps: 14, steps per second: 60, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.357 [0.000, 1.000], mean observation: 0.094 [-0.987, 1.661], loss: 0.537787, mean_absolute_error: 2.689499, mean_q: 5.146400\n",
      " 805/1000: episode: 40, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.300 [0.000, 1.000], mean observation: 0.118 [-0.995, 1.652], loss: 0.452274, mean_absolute_error: 2.713534, mean_q: 5.295782\n",
      " 814/1000: episode: 41, duration: 0.149s, episode steps: 9, steps per second: 61, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.222 [0.000, 1.000], mean observation: 0.155 [-0.983, 1.766], loss: 0.526527, mean_absolute_error: 2.759816, mean_q: 5.434955\n",
      " 823/1000: episode: 42, duration: 0.150s, episode steps: 9, steps per second: 60, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.222 [0.000, 1.000], mean observation: 0.113 [-1.223, 1.887], loss: 0.603027, mean_absolute_error: 2.857428, mean_q: 5.561079\n",
      " 832/1000: episode: 43, duration: 0.153s, episode steps: 9, steps per second: 59, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.222 [0.000, 1.000], mean observation: 0.160 [-0.948, 1.776], loss: 0.820664, mean_absolute_error: 2.944323, mean_q: 5.593601\n",
      " 843/1000: episode: 44, duration: 0.182s, episode steps: 11, steps per second: 61, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.126 [-1.136, 1.884], loss: 0.571285, mean_absolute_error: 2.909977, mean_q: 5.570744\n",
      " 852/1000: episode: 45, duration: 0.151s, episode steps: 9, steps per second: 60, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.151 [-1.332, 2.260], loss: 0.706020, mean_absolute_error: 2.978613, mean_q: 5.667314\n",
      " 862/1000: episode: 46, duration: 0.166s, episode steps: 10, steps per second: 60, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.300 [0.000, 1.000], mean observation: 0.155 [-0.949, 1.722], loss: 0.935619, mean_absolute_error: 3.068180, mean_q: 5.763212\n",
      " 872/1000: episode: 47, duration: 0.161s, episode steps: 10, steps per second: 62, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.300 [0.000, 1.000], mean observation: 0.138 [-0.942, 1.619], loss: 0.827335, mean_absolute_error: 3.057087, mean_q: 5.743443\n",
      " 882/1000: episode: 48, duration: 0.169s, episode steps: 10, steps per second: 59, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.300 [0.000, 1.000], mean observation: 0.098 [-1.015, 1.624], loss: 0.748905, mean_absolute_error: 3.093410, mean_q: 5.783143\n",
      " 894/1000: episode: 49, duration: 0.196s, episode steps: 12, steps per second: 61, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.099 [-1.180, 1.799], loss: 1.096923, mean_absolute_error: 3.177761, mean_q: 5.915624\n",
      " 905/1000: episode: 50, duration: 0.181s, episode steps: 11, steps per second: 61, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.182 [0.000, 1.000], mean observation: 0.100 [-1.393, 2.205], loss: 0.589861, mean_absolute_error: 3.145619, mean_q: 5.955770\n",
      " 915/1000: episode: 51, duration: 0.164s, episode steps: 10, steps per second: 61, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.122 [-1.144, 1.941], loss: 1.067460, mean_absolute_error: 3.247748, mean_q: 6.106978\n",
      " 928/1000: episode: 52, duration: 0.216s, episode steps: 13, steps per second: 60, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.308 [0.000, 1.000], mean observation: 0.096 [-1.018, 1.682], loss: 0.839071, mean_absolute_error: 3.264117, mean_q: 6.160448\n",
      " 943/1000: episode: 53, duration: 0.253s, episode steps: 15, steps per second: 59, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.088 [-0.831, 1.401], loss: 0.991174, mean_absolute_error: 3.345458, mean_q: 6.155046\n",
      " 955/1000: episode: 54, duration: 0.196s, episode steps: 12, steps per second: 61, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.417 [0.000, 1.000], mean observation: 0.099 [-0.792, 1.338], loss: 1.128123, mean_absolute_error: 3.418546, mean_q: 6.210066\n",
      " 967/1000: episode: 55, duration: 0.198s, episode steps: 12, steps per second: 61, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.417 [0.000, 1.000], mean observation: 0.104 [-0.763, 1.222], loss: 0.754330, mean_absolute_error: 3.367833, mean_q: 6.245075\n",
      " 981/1000: episode: 56, duration: 0.232s, episode steps: 14, steps per second: 60, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.357 [0.000, 1.000], mean observation: 0.079 [-1.014, 1.677], loss: 0.886188, mean_absolute_error: 3.407130, mean_q: 6.414288\n",
      " 992/1000: episode: 57, duration: 0.182s, episode steps: 11, steps per second: 60, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.364 [0.000, 1.000], mean observation: 0.106 [-0.803, 1.384], loss: 1.050773, mean_absolute_error: 3.484076, mean_q: 6.512092\n",
      "done, took 17.732 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2b0421e4710>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy = EpsGreedyQPolicy()\n",
    "memory = SequentialMemory(limit=50000, window_length=1)\n",
    "dqn = DQNAgent(model=model, nb_actions=nb_actions, memory=memory, nb_steps_warmup=10,\n",
    "target_model_update=1e-2, policy=policy)\n",
    "dqn.compile(Adam(lr=1e-3), metrics=['mae'])\n",
    "\n",
    "# Okay, now it's time to learn something! We visualize the training here for show, but this slows down training quite a lot. \n",
    "dqn.fit(env, nb_steps=1000, visualize=True, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 50 episodes ...\n",
      "Episode 1: reward: 12.000, steps: 12\n",
      "Episode 2: reward: 13.000, steps: 13\n",
      "Episode 3: reward: 12.000, steps: 12\n",
      "Episode 4: reward: 13.000, steps: 13\n",
      "Episode 5: reward: 10.000, steps: 10\n",
      "Episode 6: reward: 12.000, steps: 12\n",
      "Episode 7: reward: 12.000, steps: 12\n",
      "Episode 8: reward: 10.000, steps: 10\n",
      "Episode 9: reward: 9.000, steps: 9\n",
      "Episode 10: reward: 11.000, steps: 11\n",
      "Episode 11: reward: 10.000, steps: 10\n",
      "Episode 12: reward: 10.000, steps: 10\n",
      "Episode 13: reward: 11.000, steps: 11\n",
      "Episode 14: reward: 13.000, steps: 13\n",
      "Episode 15: reward: 9.000, steps: 9\n",
      "Episode 16: reward: 11.000, steps: 11\n",
      "Episode 17: reward: 12.000, steps: 12\n",
      "Episode 18: reward: 11.000, steps: 11\n",
      "Episode 19: reward: 12.000, steps: 12\n",
      "Episode 20: reward: 12.000, steps: 12\n",
      "Episode 21: reward: 11.000, steps: 11\n",
      "Episode 22: reward: 11.000, steps: 11\n",
      "Episode 23: reward: 11.000, steps: 11\n",
      "Episode 24: reward: 12.000, steps: 12\n",
      "Episode 25: reward: 13.000, steps: 13\n",
      "Episode 26: reward: 12.000, steps: 12\n",
      "Episode 27: reward: 12.000, steps: 12\n",
      "Episode 28: reward: 13.000, steps: 13\n",
      "Episode 29: reward: 13.000, steps: 13\n",
      "Episode 30: reward: 12.000, steps: 12\n",
      "Episode 31: reward: 12.000, steps: 12\n",
      "Episode 32: reward: 9.000, steps: 9\n",
      "Episode 33: reward: 11.000, steps: 11\n",
      "Episode 34: reward: 13.000, steps: 13\n",
      "Episode 35: reward: 9.000, steps: 9\n",
      "Episode 36: reward: 10.000, steps: 10\n",
      "Episode 37: reward: 12.000, steps: 12\n",
      "Episode 38: reward: 12.000, steps: 12\n",
      "Episode 39: reward: 13.000, steps: 13\n",
      "Episode 40: reward: 13.000, steps: 13\n",
      "Episode 41: reward: 10.000, steps: 10\n",
      "Episode 42: reward: 12.000, steps: 12\n",
      "Episode 43: reward: 12.000, steps: 12\n",
      "Episode 44: reward: 12.000, steps: 12\n",
      "Episode 45: reward: 12.000, steps: 12\n",
      "Episode 46: reward: 9.000, steps: 9\n",
      "Episode 47: reward: 10.000, steps: 10\n",
      "Episode 48: reward: 11.000, steps: 11\n",
      "Episode 49: reward: 12.000, steps: 12\n",
      "Episode 50: reward: 11.000, steps: 11\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2b0410d2b00>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dqn.test(env, nb_episodes=50, visualize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
